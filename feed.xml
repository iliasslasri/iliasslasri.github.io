<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iliasslasri.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iliasslasri.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-12T08:46:15+00:00</updated><id>https://iliasslasri.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">PESTO (VQT, Losses &amp;amp; Eval on reverb)</title><link href="https://iliasslasri.github.io/blog/2026/pesto/" rel="alternate" type="text/html" title="PESTO (VQT, Losses &amp;amp; Eval on reverb)"/><published>2026-01-02T11:00:00+00:00</published><updated>2026-01-02T11:00:00+00:00</updated><id>https://iliasslasri.github.io/blog/2026/pesto</id><content type="html" xml:base="https://iliasslasri.github.io/blog/2026/pesto/"><![CDATA[<h1 id="i-vqt">I. VQT</h1> <h2 id="1-what-is-the-cqtvqt">1. What is the CQT/VQT?</h2> <p>The <strong>Constant-Q Transform (CQT)</strong> is a time-frequency representation of an audio signal. Unlike the Short-Time Fourier Transform (STFT), which has a fixed frequency resolution (linear frequency spacing), the CQT has a logarithmic frequency spacing.</p> <p>This means:</p> <ul> <li><strong>Low frequencies</strong> are analyzed with high frequency resolution (narrow bandwidth filters) but poor time resolution, requires large analysis window. Note that this can be limitation in real time inference.</li> <li><strong>High frequencies</strong> are analyzed with low frequency resolution (wide bandwidth filters) but high time resolution.</li> <li>The ratio of center frequency to bandwidth ($Q$) remains <strong>constant</strong> for all bins.</li> </ul> <p>This structure mimics the human auditory system and the musical scale, where notes are spaced logarithmically (e.g., octaves are powers of 2).</p> <p>The center frequency for the $k$-th bin, denoted as $f_k$, follows a geometric progression:</p> \[f_k = f_{min} \cdot 2^{\frac{k}{B}}\] <p>Where:</p> <ul> <li>$k = 1, 2, …, K$ is the bin index.</li> <li>$f_{min}$ is the lowest center frequency (e.g., 27.5 Hz for the lowest piano note A0).</li> <li>$B$ is the number of bins per octave (frequency resolution).</li> </ul> <p>The <em>VQT</em> differs from the CQT y adding a parameter γ, which smoothly decreases the Q factors of the analysis filters for low frequencies, so since the Q factor is reduced for low freqs, this reduces the window lenght for these low frequencies allowing for higher frame rate.</p> <h3 id="2-key-variables-in-cqtvqt">2. Key Variables in CQT/VQT</h3> <table> <thead> <tr> <th style="text-align: left">Variable</th> <th style="text-align: left"> </th> <th style="text-align: left">Definition &amp; Formula</th> <th style="text-align: left">Typical Value</th> <th style="text-align: left">Significance &amp; Impact</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>$f_{min}$</strong></td> <td style="text-align: left"><strong>Minimum Frequency</strong></td> <td style="text-align: left">The center frequency of the very first bin.</td> <td style="text-align: left"><strong>27.5 Hz</strong> (A0 on piano)</td> <td style="text-align: left">It sets the “anchor” for the entire logarithmic scale.</td> </tr> <tr> <td style="text-align: left"><strong>$B$</strong></td> <td style="text-align: left"><strong>Bins per Octave</strong></td> <td style="text-align: left">Defines the resolution of the transform (frequency bands per octave).<br/>If $b$ is bins per semitone*, then $B = 12 \times b$.</td> <td style="text-align: left"><strong>$B=36$</strong> ($b=3$)</td> <td style="text-align: left">Higher $B$ gives better frequency precision but requires longer computation windows, resulting in worse time precision.</td> </tr> <tr> <td style="text-align: left"><strong>$Q$</strong></td> <td style="text-align: left"><strong>Quality Factor</strong></td> <td style="text-align: left">The ratio of a filter’s center frequency to its bandwidth (constant for all $k$).<br/>Formula: \(Q = \frac{f_k}{\Delta f_k} = \frac{1}{2^{1/B} - 1}\)</td> <td style="text-align: left">Dependent on $B$</td> <td style="text-align: left">This constant ratio ensures that if you go up an octave, the bandwidth also doubles, keeping the “musical” resolution consistent.</td> </tr> <tr> <td style="text-align: left"><strong>$w_k$</strong></td> <td style="text-align: left"><strong>Window Length</strong></td> <td style="text-align: left">The length of the analysis window (in samples) for bin $k$.<br/>Formula: \(w_k = \frac{f_s}{f_k} \cdot Q\)</td> <td style="text-align: left">Varies with $f_k$</td> <td style="text-align: left"><strong>Low freq:</strong> Very long window (high freq resolution).<br/><strong>High freq:</strong> Very short window (high time resolution).<br/>This variation is the main difference from STFT.</td> </tr> <tr> <td style="text-align: left"><strong>$\gamma$</strong></td> <td style="text-align: left"><strong>Gamma</strong></td> <td style="text-align: left">A parameter in <strong>VQT</strong> to limit window length at low frequencies.<br/>Formula: \(w_k = \left\lceil \frac{Q \cdot f_s}{f_k + \frac{\gamma}{\zeta}} \right\rceil\)</td> <td style="text-align: left"><strong>$\gamma=7$</strong></td> <td style="text-align: left"><strong>If $\gamma = 0$:</strong> It is a standard CQT.<br/><strong>If $\gamma &gt; 0$:</strong> Bandwidths at low frequencies are artificially widened (reducing $Q$) to prevent excessive window lengths. This improves time resolution and speed.</td> </tr> </tbody> </table> <p><em>*Note: A semitone is the smallest standard musical interval in Western music, representing the distance between two adjacent notes on a piano.</em></p> <h2 id="3-the-transform-steps">3. The Transform Steps</h2> <h4 id="a-constructing-the-filters-kernels">a. Constructing the Filters (Kernels)</h4> <p>The CQT is essentially a <strong>filter bank</strong>. We need to construct a separate complex filter (kernel) for every frequency bin $k$.</p> <h5 id="step-a-determine-window-lengths">Step A: Determine Window Lengths</h5> <p>First, for each bin $k$, we calculate the specific window length $N_k$ (in samples). This depends on whether we are using CQT or VQT.</p> <ul> <li><strong>For CQT ($\gamma = 0$):</strong> \(N_k = \frac{f_s}{f_k} \cdot Q\)</li> <li><strong>For VQT ($\gamma &gt; 0$):</strong> \(N_k = \left\lceil \frac{f_s \cdot Q}{f_k + \frac{\gamma}{\zeta}} \right\rceil\)</li> </ul> <p><em>(Recall that $Q = (2^{1/B} - 1)^{-1}$ and $f_k = f_{min} \cdot 2^{k/B}$)</em>.</p> <h5 id="step-b-the-time-domain-kernel-expression">Step B: The Time-Domain Kernel Expression</h5> <p>The filter $h_k[n]$ for the $k$-th bin is a complex sinusoid modulated by a window function. It is constructed as follows:</p> \[h_k[n] = \frac{1}{C_k} \cdot w\left(\frac{n}{N_k}\right) \cdot e^{-j 2 \pi \frac{f_k}{f_s} n}\] <p>Where:</p> <ul> <li><strong>$n$</strong>: The time sample index, centered around zero, typically ranging from $-\lfloor \frac{N_k}{2} \rfloor$ to $+\lfloor \frac{N_k}{2} \rfloor$.</li> <li><strong>$e^{-j 2 \pi \dots}$</strong>: The complex exponential (sinusoid) that “tunes” this filter to detect frequency $f_k$.</li> <li><strong>$w(t)$</strong>: A window function (typically a <strong>Hamming</strong> or <strong>Hann</strong> window) centered at 0. This limits the filter in time and reduces spectral leakage.</li> <li><strong>$C_k$</strong>: A normalization constant (usually the $L_2$ norm or $L_1$ norm of the window) to ensuring energy is preserved across different bin widths.</li> </ul> <p>In implementations like <code class="language-plaintext highlighter-rouge">nnAudio</code> (used in PESTO), these kernels are <strong>pre-computed</strong> and stored as weights in a 1D Convolutional layer.</p> <hr/> <h4 id="b-performing-the-transform">b. Performing the Transform</h4> <p>Once the bank of filters ${h_k}$ is constructed for all $k = 1 \dots K$, the transform is performed via <strong>convolution</strong>.</p> <h5 id="the-expression">The Expression</h5> <p>For an input audio signal $x[n]$, the CQT coefficient $X[k, n]$ for bin $k$ at time $n$ is the result of convolving the signal with the complex conjugate of the time-reversed filter kernel:</p> \[X[k, n] = \sum_{m} x[m] \cdot h_k^*[m - n]\] <p>In practice, this is equivalent to running the audio through a 1D Convolutional Neural Network layer where:</p> <ul> <li><strong>Input:</strong> The raw audio waveform (size $1 \times T$).</li> <li><strong>Weights:</strong> The pre-computed CQT kernels (size $K \times \text{max}(N_k)$).</li> <li><strong>Output:</strong> The complex CQT spectrogram.</li> </ul> <hr/> <h1 id="ii-loss-functions">II. Loss Functions</h1> <p>PESTO minimizes a composite objective function that combines three distinct losses to enforce physical equivariance while preventing model collapse.</p> <h4 id="1-equivariance-loss-mathcall_textequiv">1. Equivariance Loss ($\mathcal{L}_{\text{equiv}}$)</h4> <p><strong>Purpose:</strong> Enforces the geometric property that a pitch shift in the input must result in a proportional shift in the output probability distribution.</p> <p>Instead of using a decoder, PESTO projects the output distribution $y$ onto a scalar using a deterministic linear form $\phi$. If $y^{(k)}$ is a valid $k$-transposition of $y$, the ratio of their projections must equal $\alpha^k$ (where $\alpha = 2^{1/36}$ in practice).</p> <p>The loss minimizes the error between the actual projected ratio and the expected ratio using the Huber loss function $h_\tau$ for robustness:</p> \[\mathcal{L}_{\text{equiv}}(y, y^{(k)}, k) = h_\tau \left( \frac{\phi(y^{(k)})}{\phi(y)} - \alpha^k \right)\] <h4 id="2-shifted-cross-entropy-loss-mathcall_textsce">2. Shifted Cross-Entropy Loss ($\mathcal{L}_{\text{SCE}}$)</h4> <p><strong>Purpose:</strong> Acts as a regularization term to enforce the shape of the distribution and prevent the model from satisfying equivariance trivially (e.g., by outputting flat distributions).</p> <p>This loss explicitly compares the original output $y$ against the shifted output $y^{(k)}$. Since a shift of $k$ bins moves some indices out of the viewable frame, these “out-of-bounds” indices are replaced by 0 (masked). This ensures the model is not penalized for pitches that disappear from the frequency range due to the shift.</p> \[\mathcal{L}_{\text{SCE}}(y, y^{(k)}, k) = \sum_{i=0}^{d-1} y_i \log(y^{(k)}_{i+k})\] <p><em>(Note: Indices $i+k$ that fall outside the valid range $[0, d-1]$ are ignored in the sum)</em>.</p> <h4 id="3-invariance-loss-mathcall_textinv">3. Invariance Loss ($\mathcal{L}_{\text{inv}}$)</h4> <p><strong>Purpose:</strong> Ensures the model predicts pitch based on frequency content rather than timbre or loudness.</p> <p>We generates augmented views $\tilde{x}$ of the input $x$ using pitch-preserving transforms (gain, additive white noise, or background mixing). The loss then minimizes the standard cross-entropy between the predictions of the original and augmented views.</p> \[\mathcal{L}_{\text{inv}}(y, \tilde{y}) = \text{CrossEntropy}(y, \tilde{y})\] <h2 id="loss-symmetry--gradient-stopping">Loss Symmetry &amp; Gradient Stopping</h2> <p>The PESTO loss function incorporates three critical mechanisms to ensure effective SSL: <strong>Symmetry</strong>, <strong>Stop Gradient</strong> and <strong>Loss Weighting</strong>.</p> <h4 id="1-why-losses-should-be-symmetric">1. Why losses should be symmetric</h4> <p>Since PESTO is self-supervised, there is no “ground truth” label. The model learns by comparing two different augmented views of the same input (e.g., $y$ and $\tilde{y}$), and standard Cross-Entropy $\text{Loss}(A, B)$ is directional, it assumes $B$ is the truth and optimizes $A$ to match it. So to treat both views equally, the loss is calculated in both directions and averaged. This ensures the model learns a robust representation regardless of which augmented view is presented as the “target.”</p> \[\text{Loss} = \frac{1}{2} \left[ \underbrace{\mathcal{L}(y, \tilde{y})}_{\text{Predict } y \text{ from } \tilde{y}} + \underbrace{\mathcal{L}(\tilde{y}, y)}_{\text{Predict } \tilde{y} \text{ from } y} \right]\] <h4 id="2-why-stop-gradient-is-used">2. Why stop gradient is used</h4> <p>The “Stop Gradient” operation is a stability mechanism widely used in Siamese networks (like SimSiam or BYOL) to prevent Model Collapse.</p> <p>If the network updates its parameters to minimize the distance between $y$ and $\tilde{y}$ simultaneously, it may converge on a “lazy” solution, so the network could learn to output a constant vector (e.g., all zeros or a uniform distribution) for every input. If $y = \tilde{y} = \text{constant}$, the loss is zero, but the model has learned nothing about pitch. That’s why we artificially freeze one side of the equation during backpropagation.</p> <ul> <li>In the term $\mathcal{L}(y, \text{sg}(\tilde{y}))$, $\tilde{y}$ is treated as a <strong>fixed constant</strong>.</li> <li>The network only updates weights to move $y$ closer to $\tilde{y}$, not to move $\tilde{y}$ closer to $y$.</li> <li><strong>Result:</strong> This prevents the degenerate scenario where both outputs simply “meet in the middle” at a meaningless value, forcing the network to learn meaningful transformations.</li> </ul> <h4 id="3-adaptive-loss-weighting">3. Adaptive Loss Weighting</h4> <p>If one loss is numerically much larger than the others, the model will focus only on that one and ignore the others, and since we are not sure that the three losses have the same scales and gradients. To balance the three competing objectives, PESTO does not use fixed hyperparameters. Instead, it employs <strong>GradNorm</strong>, a gradient-based normalization algorithm, since the norm of the gradient of each loss can be interpreted as its contribution to the total objective to optimize.. Refer to the two papers: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf">Taming Transformers for High-Resolution Image Synthesis</a> and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/4eb2c0adafbe71269f3a772c130f9e53-Paper-Conference.pdf">Value Function Decomposition for Iterative Design of Reinforcement Learning Agents</a>.</p> <h2 id="iii-experimental-results--robustness">III. Experimental Results &amp; Robustness</h2> <p>my work extended the original PESTO evaluation to include <strong>Environmental Robustness</strong> (Reverberation).</p> <h3 id="robustness-to-reverberation">Robustness to Reverberation</h3> <p>To evaluate how PESTO performs in real-world acoustic environments, we subjected the model to synthetic reverberation. This simulates the effect of sound reflections in a physical space, which can obscure the fundamental frequency.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reverb_filter-480.webp 480w,/assets/img/reverb_filter-800.webp 800w,/assets/img/reverb_filter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/reverb_filter.png" class="img-fluid" width="100%" height="auto" title="Comparaison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="quantitative-analysis">Quantitative Analysis</h4> <p>We tested the model using two datasets—<strong>MIR-1K</strong> (singing voice) and <strong>MDB</strong> (musical instruments)—under varying reverb mix levels.</p> <table> <thead> <tr> <th>Mix</th> <th>RPA (MIR-1K)</th> <th>RPA (MDB)</th> <th>RCA (MIR-1K)</th> <th>RCA (MDB)</th> </tr> </thead> <tbody> <tr> <td><strong>0.0 (Clean)</strong></td> <td>86.92%</td> <td>88.21%</td> <td>87.19%</td> <td>92.25%</td> </tr> <tr> <td><strong>0.3 (Mild)</strong></td> <td>83.56%</td> <td>83.28%</td> <td>83.83%</td> <td>88.62%</td> </tr> <tr> <td><strong>0.6 (Heavy)</strong></td> <td><strong>69.32%</strong></td> <td><strong>69.78%</strong></td> <td>70.13%</td> <td>76.11%</td> </tr> <tr> <td><strong>0.9 (Extreme)</strong></td> <td>59.61%</td> <td>62.87%</td> <td>60.50%</td> <td>69.13%</td> </tr> </tbody> </table> <ul> <li><strong>Stability:</strong> The model is remarkably stable under mild reverb conditions.</li> <li><strong>Degradation:</strong> A sharp performance drop occurs at , where the reflections begin to dominate the direct signal.</li> </ul> <h4 id="qualitative-error-analysis">Qualitative Error Analysis</h4> <p>When examining the predictions at extreme reverb levels, we observed specific failure modes:</p> <div class="row"> <div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pesto_0.9-480.webp 480w,/assets/img/pesto_0.9-800.webp 800w,/assets/img/pesto_0.9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pesto_0.9.png" class="img-fluid" width="100%" height="auto" title="Comparaison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Octave Errors:</strong> Reverb tails often create constructive interference at higher harmonics. If the second harmonic becomes more energetic than the fundamental due to reflections, the model may mistakenly track the higher octave.</li> <li><strong>Temporal Smearing:</strong> Reverberation causes spectral energy to persist long after the note has ended. This “smearing” creates misalignment between the actual pitch and the model’s prediction.</li> <li><strong>Structural Limitations:</strong> Because PESTO’s architecture relies on <strong>1D-convolutions along the frequency axis</strong>, it processes frames independently. It lacks the <strong>temporal memory</strong> (e.g., recurrent layers or attention) necessary to distinguish between a new note and the “echo” of a previous one.</li> </ul>]]></content><author><name></name></author><category term="explanation"/><category term="signal-processing"/><category term="audio"/><category term="mir"/><category term="deep-learning"/><summary type="html"><![CDATA[Pitch Estimation with Self-supervised Transposition-Equivariant Objective]]></summary></entry><entry><title type="html">A Curated Collection of AI &amp;amp; Audio Resources</title><link href="https://iliasslasri.github.io/blog/2025/resources/" rel="alternate" type="text/html" title="A Curated Collection of AI &amp;amp; Audio Resources"/><published>2025-12-17T13:00:00+00:00</published><updated>2025-12-17T13:00:00+00:00</updated><id>https://iliasslasri.github.io/blog/2025/resources</id><content type="html" xml:base="https://iliasslasri.github.io/blog/2025/resources/"><![CDATA[<p>This is a living collection of resources, papers, and tools that I have found useful for Deep Learning, specifically focusing on Audio ML, LLMs, and ML Systems efficiency.</p> <h2 id="good-blogs">Good blogs</h2> <ul> <li><strong><a href="https://magazine.sebastianraschka.com/">Ahead of AI by sebastian raschka</a></strong>.</li> </ul> <h2 id="audio-ml--signal-processing">Audio ML &amp; Signal Processing</h2> <p>A mix of fundamental theory and state-of-the-art applications in audio.</p> <ul> <li><strong><a href="https://www.soundsandwords.io/audio-loss-functions/">Loss Functions in Audio ML</a></strong>: A great overview of how we measure error in audio domains.</li> <li><strong><a href="https://arxiv.org/pdf/1904.08369">Deep Filtering Paper</a></strong>: Essential reading for speech enhancement.</li> <li><strong><a href="https://github.com/linto-ai/whisper-timestamped">Whisper Timestamped</a></strong>: An extension for Multilingual Automatic Speech Recognition that adds word-level timestamps and confidence scores.</li> <li><strong><a href="https://distill.pub/2017/ctc/">Sequence Modeling With CTC</a></strong>: An interactive Distill.pub article explaining Connectionist Temporal Classification.</li> </ul> <h3 id="books--theory">Books &amp; Theory</h3> <ul> <li><strong><a href="https://musicinformationretrieval.com/">Music Information Retrieval (MIR)</a></strong>: A comprehensive guide to MIR.</li> <li><strong><a href="https://brianmcfee.net/dstbook-site/content/intro.html">Digital Signals Theory</a></strong>: An accessible introduction by Brian McFee.</li> </ul> <hr/> <h2 id="transformers--llms">Transformers &amp; LLMs</h2> <p>Resources to understand the backbone of modern AI, from attention mechanisms to fine-tuning strategies.</p> <h3 id="architecture-internals">Architecture Internals</h3> <ul> <li><strong><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></strong>: Jay Alammar’s classic visual guide.</li> <li><strong><a href="https://arxiv.org/pdf/2207.09238">Formal Algorithms for Transformers</a></strong>: DeepMind’s rigorous formalization of the architecture.</li> <li><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></strong>: Understanding hardware utilization.</li> <li><strong><a href="https://huggingface.co/blog/not-lain/tensor-dims">Mastering Tensor Dimensions in Transformers</a></strong>: A guide to keeping track of shapes in Hugging Face.</li> <li><strong><a href="https://www.evanmiller.org/attention-is-off-by-one.html">Attention is off by one</a></strong>: A critical look at Softmax implementation nuances.</li> <li><strong><a href="https://eugeneyan.com/writing/attention/">Some Intuition on Attention</a></strong>: Eugene Yan’s breakdown of the mechanism.</li> <li><strong><a href="https://sh-tsang.medium.com/review-pre-ln-transformer-on-layer-normalization-in-the-transformer-architecture-b6c91a89e9ab">Pre-LN Transformer</a></strong>: A review on why Layer Normalization placement matters.</li> <li><strong><a href="https://arxiv.org/pdf/2305.13245">Grouped Query Attention</a></strong>: The paper introducing GQA for efficient inference.</li> </ul> <h3 id="large-language-models-llms">Large Language Models (LLMs)</h3> <ul> <li><strong><a href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a></strong> &amp; <strong><a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT</a></strong>: Visual guides by Jay Alammar.</li> <li><strong><a href="https://www.mikecaptain.com/resources/pdf/GPT-1.pdf">Improving Language Understanding by Generative Pre-Training (GPT-1)</a></strong>: The original paper that started the GPT series.</li> <li><strong><a href="https://huggingface.co/blog/mlabonne/decoding-strategies">Decoding Strategies in LLMs</a></strong>: An overview of greedy search, beam search, and sampling methods.</li> </ul> <h3 id="training--fine-tuning-dpo--lora">Training &amp; Fine-Tuning (DPO &amp; LoRA)</h3> <ul> <li><strong><a href="https://www.tylerromero.com/posts/2024-04-dpo/">Direct Preference Optimization (DPO) Explained</a></strong>: An in-depth look at preference optimization.</li> <li><strong><a href="https://thesalt.substack.com/p/llm-alignment-on-policy-vs-off-policy">LLM Alignment: On-Policy vs. Off-Policy</a></strong>: Understanding data selection for alignment.</li> <li><strong><a href="https://thinkingmachines.ai/blog/lora/">LoRA Without Regret</a></strong>: Insights on Low-Rank Adaptation by Thinking Machines.</li> </ul> <hr/> <h2 id="deep-learning-engineering--systems">Deep Learning Engineering &amp; Systems</h2> <p>Practical tools, hardware architectures, and foundational concepts.</p> <h3 id="engineering--frameworks">Engineering &amp; Frameworks</h3> <ul> <li><strong><a href="https://github.com/ashleve/lightning-hydra-template">Lightning + Hydra Template</a></strong>: A robust starting point for PyTorch Lightning projects.</li> <li><strong><a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">PyTorch Autograd Engine</a></strong>: An overview of how gradients are calculated under the hood.</li> <li><strong><a href="https://blog.yyliu.net/remote-tensorboard/">Remote Tensorboard</a></strong>: A tip on how to route remote visualization ports to your local machine.</li> </ul> <h3 id="core-concepts">Core Concepts</h3> <ul> <li><strong>Normalization</strong>: <a href="https://arxiv.org/abs/1502.03167">Batch Normalization (Paper)</a> &amp; <a href="https://arxiv.org/abs/1607.06450">Layer Normalization (Paper)</a>.</li> <li><strong>RNNs</strong>: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (Colah’s Blog).</li> <li><strong>Basics</strong>: <a href="https://www.youtube.com/@kilianweinberger698">Kilian Weinberger’s Course</a> (Cornell) for linear models and fundamentals.</li> </ul> <h3 id="hardware--architecture">Hardware &amp; Architecture</h3> <ul> <li><strong><a href="https://github.com/OpenXiangShan/XiangShan">XiangShan (Open Source CPU)</a></strong>: Top-performing open-source RISC-V processor. (<a href="https://docs.xiangshan.cc/zh-cn/latest/">Docs</a>).</li> </ul> <hr/> <h2 id="courses">Courses</h2> <p>Specialized coursework for deep diving into specific verticals.</p> <ul> <li><strong><a href="https://hanlab.mit.edu/courses/2024-fall-65940">MIT: TinyML and Efficient Deep Learning Computing</a></strong> (Fall 2024)</li> <li><strong><a href="https://abdelfattah-class.github.io/ece5545/">Cornell: Machine Learning Hardware and Systems</a></strong></li> <li><strong><a href="https://huggingface.co/learn/audio-course">Hugging Face: Audio Course</a></strong></li> </ul> <h3 id="useful-commands--scripts">Useful Commands &amp; scripts</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --job-name=my_job</span>
<span class="c">#SBATCH --output=result.out</span>
<span class="c">#SBATCH --partition=P100</span>
<span class="c">#SBATCH --gres=gpu:1 </span>
<span class="c">#SBATCH --time=10:00:00</span>

srun python script.py
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sinfo
sbatch job.sh
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squeue <span class="nt">-u</span> user <span class="c"># see jobs</span>
<span class="nb">tail</span> <span class="nt">-f</span> result.out
scancel &lt;JOBID&gt;
</code></pre></div></div>]]></content><author><name></name></author><category term="learning"/><category term="machine-learning"/><category term="audio"/><category term="llm"/><category term="deep-learning"/><category term="resources"/><summary type="html"><![CDATA[A comprehensive list of resources, papers, and courses covering Audio ML, Transformers, LLMs, and Efficient Deep Learning that I find valuable.]]></summary></entry><entry><title type="html">NMF Divergence Derivation by ME</title><link href="https://iliasslasri.github.io/blog/2024/nmf/" rel="alternate" type="text/html" title="NMF Divergence Derivation by ME"/><published>2024-12-10T09:30:00+00:00</published><updated>2024-12-10T09:30:00+00:00</updated><id>https://iliasslasri.github.io/blog/2024/nmf</id><content type="html" xml:base="https://iliasslasri.github.io/blog/2024/nmf/"><![CDATA[<p>The objective here is to minimize the Kullback-Leibler (KL) divergence between the original data matrix \(V\)and its approximation\(WH\):</p> \[D(V||WH) = \sum_{f,t} \left( V_{ft} \log \frac{V_{ft}}{(WH)_{ft}} - V_{ft} + (WH)_{ft} \right)\] <p>Where:</p> <ul> <li>\(V\)is the data matrix of size\(n \times m\)</li> <li>\(W\)is the basis matrix of size\(n \times K\)</li> <li>\(H\)is the activation matrix of size\(K \times m\)</li> </ul> <p>For a fixed \(H\), we can minimize the divergence with respect to \(W\) by computing the gradient:</p> \[\frac{\partial D(V||WH)}{\partial W}\] <p>Let’s compute the gradient of the divergence with respect to a single element \(W_{fk}\):</p> \[\frac{\partial D(V||WH)}{\partial W_{fk}} = \sum_{t} \left( \left( - \frac{V_{ft}}{(WH)_{ft}} + 1 \right) H_{kt}\right)\] <p>Now, supposing that we have the following gradient descent update rule for \(W\)with a learning rate\(\lambda\):</p> \[W_{fk} \leftarrow W_{fk} - \lambda_{k} \frac{\partial D(V||WH)}{\partial W_{fk}}\] <p>Because this is a constrained optimization problem (matrices \(W\)and\(H\) must remain non-negative), we use a specialized learning rate. Substituting the gradient into the update rule gives:</p> \[W_{fk} \leftarrow W_{fk} + \lambda_{k} \sum_{t} \left( \left( \frac{V_{ft}}{(WH)_{ft}} - 1 \right) H_{kt} \right)\] <p>To derive the multiplicative update rule, we choose a specific, adaptive learning rate \(\lambda_{k}\) such that the subtraction terms cancel out:</p> \[\lambda_{k} = \frac{W_{fk}}{\sum_{t} H_{kt}}\] <p>Substituting this \(\lambda_k\) back into our update equation:</p> \[W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \left( \frac{V_{ft}}{(WH)_{ft}} - 1 \right) H_{kt} \right)\] <p>Distributing the terms inside the summation:</p> \[W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}} - H_{kt} \right)\] <p>Splitting the summation:</p> \[W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} \left( \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - \sum_{t} H_{kt} \right)\] <p>Distributing the fraction:</p> \[W_{fk} \leftarrow W_{fk} + \left( \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} H_{kt} \right)\] <p>Notice that in the second term, \(\sum_{t} H_{kt}\) cancels out:</p> \[W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - W_{fk}\] <p>The \(+ W_{fk}\)and\(- W_{fk}\)cancel each other out, leaving us with the final Multiplicative Update Rule for\(W\):</p> \[W_{fk} \leftarrow \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right)\] <p>Or, in matrix notation:</p> \[W \leftarrow W \otimes \frac{\frac{V}{WH} H^T}{\mathbf{1} H^T}\]]]></content><author><name></name></author><category term="derivations"/><category term="math"/><category term="machine-learning"/><category term="nmf"/><category term="optimization"/><summary type="html"><![CDATA[A step-by-step derivation of the Multiplicative Update Rule for Non-negative Matrix Factorization using KL-Divergence.]]></summary></entry><entry><title type="html">Mimi: The Codec behind Moshi and Unmute</title><link href="https://iliasslasri.github.io/blog/2024/mimi/" rel="alternate" type="text/html" title="Mimi: The Codec behind Moshi and Unmute"/><published>2024-07-20T13:00:00+00:00</published><updated>2024-07-20T13:00:00+00:00</updated><id>https://iliasslasri.github.io/blog/2024/mimi</id><content type="html" xml:base="https://iliasslasri.github.io/blog/2024/mimi/"><![CDATA[<p><img src="../img/mimi_arch.avif" alt="Mimi architecture diagram"/></p> <p><a href="https://kyutai.org/Moshi.pdf">Official Paper</a> for reference. Minimal environment setup to use Mimi can be found <a href="https://github.com/iliasslasri/mimi">in my repo</a>.</p> <h2 id="the-scope-of-this-post">The Scope of this Post</h2> <p>Today we are going to explore Mimi, a state-of-the-art neural audio codec developed by Kyutai. We will compare audio quality before and after Mimi processing, and we will then study how Mimi works and what types of outputs it produces.</p> <h2 id="what-is-mimi">What is Mimi?</h2> <blockquote> <p>“Mimi codec is a state-of-the-art audio neural codec, developed by Kyutai, that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps.”</p> </blockquote> <p>This is the official description of Mimi from Kyutai’s Huggingface repo. Mimi is also the neural audio codec that powers Moshi (a demo is currently available on <a href="https://moshi.chat">moshi.chat</a>), which we may discuss in a future post.</p> <h2 id="what-is-a-neural-audio-codec">What is a Neural Audio Codec?</h2> <p>First, let’s define a codec: A codec, short for “coder-decoder” or “compressor-decompressor”, is a device or computer program that encodes or decodes a digital data stream or signal.</p> <p>The primary purposes of codecs are to:</p> <ul> <li>Reduce file size for efficient storage</li> <li>Decrease bandwidth requirements for transmission</li> <li>Maintain an acceptable level of quality</li> </ul> <p>An audio codec is a codec that encodes or decodes audio data. There exist two types of audio codecs:</p> <ul> <li><strong>Lossy audio codecs:</strong> These compress audio by removing some data, resulting in smaller file sizes but with some loss in quality (e.g., MP3, AAC).</li> <li><strong>Lossless audio codecs:</strong> These compress audio without losing any original data, maintaining perfect quality but with larger file sizes compared to lossy codecs (e.g., FLAC, ALAC).</li> </ul> <p>A neural audio codec is a type of audio codec that leverages DNNs, neural networks in particular, to compress and decompress audio signals. Some examples include:</p> <ul> <li><strong>Lyra:</strong> Developed by Google, designed for low-bitrate speech compression.</li> <li><strong>EnCodec:</strong> Created by Meta AI, a high-fidelity neural audio codec.</li> <li><strong>SoundStream:</strong> Another Google codec focusing on high-quality audio compression at low bitrates.</li> </ul> <p>Mimi’s architecture is mostly derived from SoundStream and EnCodec, as stated in the <a href="https://kyutai.org/Moshi.pdf">official paper</a>, although it has some novel features that set it apart.</p> <h2 id="mimi-in-detail">Mimi in Detail</h2> <p>As mentioned earlier, Mimi is a neural audio codec that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps.</p> <p>However, there is a slight discrepancy: the actual implementation of Mimi runs at <strong>12.5Hz</strong>, as stated in the paper and in the official <code class="language-plaintext highlighter-rouge">config.json</code> file (under <code class="language-plaintext highlighter-rouge">frame_rate</code>). Apart from that, Mimi is a fascinating codec that produces high-quality audio at an incredibly low bitrate.</p> <p>Being an audio codec, Mimi is composed of two main components which form a bottleneck architecture:</p> <ol> <li><strong>An encoder:</strong> Takes an audio signal as input and compresses it into a smaller representation.</li> <li><strong>A decoder:</strong> Takes the compressed representation and reconstructs the audio signal.</li> </ol> <h3 id="the-encoding-process">The Encoding Process</h3> <p>The encoder compresses the audio signal into a sequence of audio codes, split into semantic and acoustic audio tokens. Here’s how it works:</p> <ul> <li>The audio signal is split into frames, each lasting 0.08 seconds (12.5Hz frame rate).</li> <li>Each frame is passed through a convolutional neural network (CNN) and a transformer, which convert the frame into a vector of length 512.</li> <li>This vector is then passed through 8 quantizers.</li> </ul> <p>These quantizers produce 1 token per frame each:</p> <ul> <li><strong>1 quantizer</strong> for the semantic tokens. These represent the content/meaning of the audio and are trained to replicate semantic information obtained from a WavLM self-supervised audio model.</li> <li><strong>7 quantizers</strong> (according to the paper) for the acoustic tokens, which capture the style and details of the audio.</li> </ul> <p>The quantized audio tokens are concatenated to produce a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, num_quantizers, sample_rate * length_audio)</code>.</p> <h3 id="bitrate-calculation">Bitrate Calculation</h3> <p>Let’s calculate the bitrate of Mimi based on the information provided using the following parameters:</p> <ul> <li>Frame rate: \(12.5 \text{ Hz}\)</li> <li>Number of audio tokens per frame: \(8\)</li> <li>Number of bits per audio token: \(\log_2(2048) = 11\) (since there are 2048 possible audio tokens)</li> </ul> <p>Therefore, the bitrate calculation is:</p> \[12.5 \times 8 \times 11 = 1100 \text{ bps} \text{ (or } 1.1 \text{ kbps)}\] <p><em>Note: Here we find another discrepancy. The paper states that there are 8 audio tokens per frame, but the official implementation produces 32 audio tokens per frame, and we can even set the number of levels in RQV as specified in <a href="https://github.com/kyutai-labs/moshi/issues/122">issue #122</a>.</em></p> <h3 id="the-decoding-process">The Decoding Process</h3> <p>The decoder takes the quantized tokens and reconstructs the audio signal following these steps:</p> <ol> <li>The tokens are passed through an inverse quantization process.</li> <li>The resulting embeddings are processed by another transformer network.</li> <li>Finally, a decoder CNN (mirroring the encoder’s CNN) reconstructs the audio waveform.</li> </ol> <h2 id="applications-of-mimi">Applications of Mimi</h2> <p>Mimi was primarily developed to power <strong>Moshi</strong>, a speech-to-speech AI model.</p> <p>Current chat models (like GPT) primarily operate on text tokens—discrete numerical representations of words. These models aren’t inherently designed to process audio data. Mimi bridges this gap by converting audio into discrete tokens, similar to how text is tokenized.</p> <p>This clever approach allows text-only models to support audio understanding and generation without requiring a complete overhaul of their architecture. Theoretically, we could fine-tune existing text-trained models to work with audio by teaching them to interpret these audio tokens alongside text tokens.</p> <h2 id="conclusions">Conclusions</h2> <p>Mimi represents a significant step forward in the field of neural audio codecs. Its ability to compress audio to incredibly low bitrates while maintaining good quality is impressive. The separation of semantic and acoustic information into distinct tokens is a novel approach that opens up new possibilities for audio processing.</p> <p>Using RQV is also a novel approach for quantization in codecs; check this for an implementation of RQV in <a href="https://github.com/lucidrains/vector-quantize-pytorch">PyTorch</a>.</p>]]></content><author><name></name></author><category term="technology"/><category term="audio"/><category term="ai"/><category term="deep-learning"/><category term="codecs"/><summary type="html"><![CDATA[A deep dive into Kyutai's state-of-the-art neural audio codec, exploring its architecture, quantization, and integration with LLMs.]]></summary></entry></feed>