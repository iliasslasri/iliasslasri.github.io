---
layout: post
title: NMF Divergence Derivation by ME
date: 2024-12-10 10:30:00 +0100
description: A step-by-step derivation of the Multiplicative Update Rule for Non-negative Matrix Factorization using KL-Divergence.
tags: math machine-learning nmf optimization
categories: derivations
related_posts: false
---



The objective here is to minimize the Kullback-Leibler (KL) divergence between the original data matrix $$V$$and its approximation$$WH$$:

$$
D(V||WH) = \sum_{f,t} \left( V_{ft} \log \frac{V_{ft}}{(WH)_{ft}} - V_{ft} + (WH)_{ft} \right)
$$

Where:
* $$V$$is the data matrix of size$$n \times m$$
* $$W$$is the basis matrix of size$$n \times K$$
* $$H$$is the activation matrix of size$$K \times m$$

For a fixed $$H$$, we can minimize the divergence with respect to $$W$$ by computing the gradient:

$$
\frac{\partial D(V||WH)}{\partial W}
$$

Let's compute the gradient of the divergence with respect to a single element $$W_{fk}$$:

$$
\frac{\partial D(V||WH)}{\partial W_{fk}} = \sum_{t} \left( \left( - \frac{V_{ft}}{(WH)_{ft}} + 1 \right) H_{kt}\right)
$$

Now, supposing that we have the following gradient descent update rule for $$W$$with a learning rate$$\lambda$$:

$$
W_{fk} \leftarrow W_{fk} - \lambda_{k} \frac{\partial D(V||WH)}{\partial W_{fk}}
$$

Because this is a constrained optimization problem (matrices $$W$$and$$H$$ must remain non-negative), we use a specialized learning rate. Substituting the gradient into the update rule gives:

$$
W_{fk} \leftarrow W_{fk} + \lambda_{k} \sum_{t} \left( \left( \frac{V_{ft}}{(WH)_{ft}} - 1 \right) H_{kt} \right)
$$

To derive the multiplicative update rule, we choose a specific, adaptive learning rate $$\lambda_{k}$$ such that the subtraction terms cancel out:

$$
\lambda_{k} = \frac{W_{fk}}{\sum_{t} H_{kt}}
$$

Substituting this $$\lambda_k$$ back into our update equation:

$$
W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \left( \frac{V_{ft}}{(WH)_{ft}} - 1 \right) H_{kt} \right)
$$

Distributing the terms inside the summation:

$$
W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} 
\sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}} - H_{kt} \right)
$$

Splitting the summation:

$$
W_{fk} \leftarrow W_{fk} + \frac{W_{fk}}{\sum_{t} H_{kt}} 
\left( \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - \sum_{t} H_{kt} \right) 
$$

Distributing the fraction:

$$
W_{fk} \leftarrow W_{fk} +
\left( \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} \left( \frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - \frac{W_{fk}}{\sum_{t} H_{kt}} \sum_{t} H_{kt} \right) 
$$

Notice that in the second term, $$\sum_{t} H_{kt}$$ cancels out:

$$
W_{fk} \leftarrow W_{fk} +
\frac{W_{fk}}{\sum_{t} H_{kt}}  \sum_{t} \left(  
\frac{V_{ft}H_{kt}}{(WH)_{ft}}\right) - W_{fk} 
$$

The $$+ W_{fk}$$and$$- W_{fk}$$cancel each other out, leaving us with the final Multiplicative Update Rule for$$W$$:

$$
W_{fk} \leftarrow 
\frac{W_{fk}}{\sum_{t} H_{kt}}  \sum_{t} \left(  
\frac{V_{ft}H_{kt}}{(WH)_{ft}}\right)
$$

Or, in matrix notation:

$$
W \leftarrow W \otimes \frac{\frac{V}{WH} H^T}{\mathbf{1} H^T}
$$